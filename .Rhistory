}
# reflect and log
for (i in 1:length(rlog)) {
d[rlog[i]] <- log(max(raw[rlog[i]],na.rm=T)+1 - raw[rlog[i]])
}
# reflect and inverse
for (i in 1:length(rinv)) {
d[rinv[i]] <- 1/(max(raw[rinv[i]])+1-raw[rinv[i]])
}
# make function to fix scale() function so it returns a useful vector not a stupid matrix
scale2 <- function(x){as.vector(scale(x, scale = FALSE))}
# scale all numeric variables
d2 <- d %>% mutate(across(.cols=duration:peakf, .fns = scale2))
# filter out calls without fundfreq measures
d3 <- d2 %>%
filter(indicator == TRUE)
write.csv(d3, "/Users/jkvrtilek/Desktop/calls_nonzeromin_timelog.csv")
rm(list = ls.str(mode = 'numeric'))
rm(list = ls.str(mode = 'character'))
rm(untrans)
rm(d)
rm(d2)
rm(raw)
raw <- d3
rm(d3)
# get sample sizes
d <- raw %>%
separate(sound.files, into=c('ds','date', 'bat', 'file', 'call'), sep="_", remove = FALSE) %>%
group_by(bat) %>%
mutate(sample.size= n()) %>%
ungroup()
# look at sample sizes
sort(unique(d$sample.size))
# set minimum sample size
# good rule of thumb is 20 obs per variable
predictors <- ncol(raw) - 3
min.sample.size <- predictors * 20
# filter calls by min sample size
d2 <- d %>%
filter(sample.size > min.sample.size)
View(d2)
n(unique(d2$bat))
count(unique(d2$bat))
unique(d2$bat)
# classify calls to bat using dfa with cross-validation (leave one-out classification)
dfa <- lda(bat ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf+
maxslope+
minslope+
abs_minslope+
pos_slopes+
neg_slopes+
turns+
meanslope+
segments,
CV=T,
data=d2)
View(d2)
unique(d2$turns)
View(raw)
raw <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/LFS/vampire_call_measures_filtered.csv")
# make df to modify
d <- raw
# how to transform variables
wsqrt <- c("dfrange","abs_minslope","pos_slopes","neg_slopes")
View(d)
range(d$turns)
range(raw$turns)
raw$turns
range(raw$turns, na.rm = T)
# how to transform variables
wsqrt <- c("dfrange","abs_minslope","pos_slopes","neg_slopes")
wlog <- c("duration","time.median","time.Q25","time.Q75","time.IQR","skew","kurt","sfm","mindom","enddom","meanpeakf","turns")
winv <- c("freq.Q25","meandom","modindx")
rsqrt <- c("freq.median")
rlog <- c("freq.Q75","freq.IQR","segments")
rinv <- c("sp.ent","time.ent","entropy")
none <- c("meanfreq","sd","maxdom","startdom","dfslope","peakf","maxslope","minslope","meanslope")
# square root
for (i in 1:length(wsqrt)) {
d[wsqrt[i]] <- sqrt(raw[wsqrt[i]])
}
# log
for (i in 1:length(wlog)) {
d[wlog[i]] <- log(raw[wlog[i]]+min(raw[wlog[i]][raw[wlog[i]] > 0], na.rm = T))
}
# manual for turns
d$turns <- log(raw$turns+1)
# inverse
for (i in 1:length(winv)) {
d[winv[i]] <- 1/raw[winv[i]]
}
# reflect and root
for (i in 1:length(rsqrt)) {
d[rsqrt[i]] <- sqrt(max(raw[rsqrt[i]])+1 - raw[rsqrt[i]])
}
# reflect and log
for (i in 1:length(rlog)) {
d[rlog[i]] <- log(max(raw[rlog[i]],na.rm=T)+1 - raw[rlog[i]])
}
# reflect and inverse
for (i in 1:length(rinv)) {
d[rinv[i]] <- 1/(max(raw[rinv[i]])+1-raw[rinv[i]])
}
# make function to fix scale() function so it returns a useful vector not a stupid matrix
scale2 <- function(x){as.vector(scale(x, scale = FALSE))}
# scale all numeric variables
d2 <- d %>% mutate(across(.cols=duration:peakf, .fns = scale2))
# filter out calls without fundfreq measures
d3 <- d2 %>%
filter(indicator == TRUE)
write.csv(d3,"/Users/jkvrtilek/Desktop/calls_nonzeromin_timelog.csv")
rm(list = ls.str(mode = 'numeric'))
rm(list = ls.str(mode = 'character'))
rm(c(d,d2,raw))
rm(d)
rm(d2)
rm(raw)
raw <- d3
rm(d3)
# load packages
library(MASS)
# get sample sizes
d <- raw %>%
separate(sound.files, into=c('ds','date', 'bat', 'file', 'call'), sep="_", remove = FALSE) %>%
group_by(bat) %>%
mutate(sample.size= n()) %>%
ungroup()
# look at sample sizes
sort(unique(d$sample.size))
# set minimum sample size
# good rule of thumb is 20 obs per variable
predictors <- ncol(raw) - 3
min.sample.size <- predictors * 20
# filter calls by min sample size
d2 <- d %>%
filter(sample.size > min.sample.size)
# classify calls to bat using dfa with cross-validation (leave one-out classification)
dfa <- lda(bat ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf+
maxslope+
minslope+
abs_minslope+
pos_slopes+
neg_slopes+
turns+
meanslope+
segments,
CV=T,
data=d2)
# get classification matrix
cm <- table(d2$bat, dfa$class)
# get overall correct classification rate (accuracy)
# this is the best accuracy estimate
correct.cases <- sum(diag(cm))
all.cases <- sum(cm)
accuracy <- correct.cases/all.cases
accuracy
# get mean and range of correct assignments/all assignments to each bat
range(diag(cm)/colSums(cm), na.rm=T)
mean(diag(cm)/colSums(cm), na.rm=T)
median(diag(cm)/colSums(cm), na.rm=T)
# get mean and range of correct assignments/all calls from each bat
range(diag(cm)/rowSums(cm), na.rm=T)
mean(diag(cm)/rowSums(cm), na.rm=T)
# classify calls to bat using dfa with cross-validation (leave one-out classification)
dfa <- lda(bat ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf+
maxslope+
minslope+
abs_minslope+
pos_slopes+
neg_slopes+
turns+
meanslope+
segments,
CV=T,
data=d2)
median(diag(cm)/rowSums(cm), na.rm=T)
raw <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/LFS/vampire_call_measures_filtered.csv")
# make df to modify
d <- raw
# temp make time variables into percentages
d$time.Q25 <- raw$time.Q25/raw$duration
d$time.median <- raw$time.median/raw$duration
d$time.Q75 <- raw$time.Q75/raw$duration
d$time.IQR <- raw$time.IQR/raw$duration
# how to transform variables
wsqrt <- c("dfrange","abs_minslope","pos_slopes","neg_slopes")
wlog <- c("duration","skew","kurt","sfm","mindom","enddom","meanpeakf","turns")
winv <- c("freq.Q25","meandom","modindx")
rsqrt <- c("freq.median")
rlog <- c("freq.Q75","freq.IQR","segments")
rinv <- c("sp.ent","time.ent","entropy")
none <- c("meanfreq","sd","maxdom","startdom","dfslope","peakf","maxslope","minslope","meanslope")
time <- c("time.median","time.Q25","time.Q75","time.IQR")
# square root
for (i in 1:length(wsqrt)) {
d[wsqrt[i]] <- sqrt(raw[wsqrt[i]])
}
# log
for (i in 1:length(wlog)) {
d[wlog[i]] <- log(raw[wlog[i]]+min(raw[wlog[i]][raw[wlog[i]] > 0], na.rm = T))
}
# inverse
for (i in 1:length(winv)) {
d[winv[i]] <- 1/raw[winv[i]]
}
# reflect and root
for (i in 1:length(rsqrt)) {
d[rsqrt[i]] <- sqrt(max(raw[rsqrt[i]])+1 - raw[rsqrt[i]])
}
# reflect and log
for (i in 1:length(rlog)) {
d[rlog[i]] <- log(max(raw[rlog[i]],na.rm=T)+1 - raw[rlog[i]])
}
# reflect and inverse
for (i in 1:length(rinv)) {
d[rinv[i]] <- 1/(max(raw[rinv[i]])+1-raw[rinv[i]])
}
# make function to fix scale() function so it returns a useful vector not a stupid matrix
scale2 <- function(x){as.vector(scale(x, scale = FALSE))}
# scale all numeric variables
d2 <- d %>% mutate(across(.cols=duration:peakf, .fns = scale2))
# filter out calls without fundfreq measures
d3 <- d2 %>%
filter(indicator == TRUE)
write.csv(d3,"/Users/jkvrtilek/Desktop/time_percent_log_duration.csv")
rm(list = ls.str(mode = 'numeric'))
rm(list = ls.str(mode = 'character'))
rm(d)
rm(d2)
rm(raw)
raw <- d3
rm(d3)
# get sample sizes
d <- raw %>%
separate(sound.files, into=c('ds','date', 'bat', 'file', 'call'), sep="_", remove = FALSE) %>%
group_by(bat) %>%
mutate(sample.size= n()) %>%
ungroup()
# look at sample sizes
sort(unique(d$sample.size))
# set minimum sample size
# good rule of thumb is 20 obs per variable
predictors <- ncol(raw) - 3
min.sample.size <- predictors * 20
# filter calls by min sample size
d2 <- d %>%
filter(sample.size > min.sample.size)
# classify calls to bat using dfa with cross-validation (leave one-out classification)
dfa <- lda(bat ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf+
maxslope+
minslope+
abs_minslope+
pos_slopes+
neg_slopes+
turns+
meanslope+
segments,
CV=T,
data=d2)
# get classification matrix
cm <- table(d2$bat, dfa$class)
# get overall correct classification rate (accuracy)
# this is the best accuracy estimate
correct.cases <- sum(diag(cm))
all.cases <- sum(cm)
accuracy <- correct.cases/all.cases
accuracy
# get mean and range of correct assignments/all assignments to each bat
range(diag(cm)/colSums(cm), na.rm=T)
mean(diag(cm)/colSums(cm), na.rm=T)
median(diag(cm)/colSums(cm), na.rm=T)
# get mean and range of correct assignments/all calls from each bat
range(diag(cm)/rowSums(cm), na.rm=T)
mean(diag(cm)/rowSums(cm), na.rm=T)
median(diag(cm)/rowSums(cm), na.rm=T)
raw <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/LFS/vampire_call_measures_filtered.csv")
# make df to modify
d <- raw
# temp make time variables into percentages
d$time.Q25 <- raw$time.Q25/raw$duration
d$time.median <- raw$time.median/raw$duration
d$time.Q75 <- raw$time.Q75/raw$duration
d$time.IQR <- raw$time.IQR/raw$duration
# how to transform variables
wsqrt <- c("dfrange","abs_minslope","pos_slopes","neg_slopes")
wlog <- c("duration","time.median","time.Q25","time.Q75","time.IQR","skew","kurt","sfm","mindom","enddom","meanpeakf","turns")
winv <- c("freq.Q25","meandom","modindx")
rsqrt <- c("freq.median")
rlog <- c("freq.Q75","freq.IQR","segments")
rinv <- c("sp.ent","time.ent","entropy")
none <- c("meanfreq","sd","maxdom","startdom","dfslope","peakf","maxslope","minslope","meanslope")
# square root
for (i in 1:length(wsqrt)) {
d[wsqrt[i]] <- sqrt(raw[wsqrt[i]])
}
# log
for (i in 1:length(wlog)) {
d[wlog[i]] <- log(raw[wlog[i]]+min(raw[wlog[i]][raw[wlog[i]] > 0], na.rm = T))
}
# inverse
for (i in 1:length(winv)) {
d[winv[i]] <- 1/raw[winv[i]]
}
# reflect and root
for (i in 1:length(rsqrt)) {
d[rsqrt[i]] <- sqrt(max(raw[rsqrt[i]])+1 - raw[rsqrt[i]])
}
# reflect and log
for (i in 1:length(rlog)) {
d[rlog[i]] <- log(max(raw[rlog[i]],na.rm=T)+1 - raw[rlog[i]])
}
# reflect and inverse
for (i in 1:length(rinv)) {
d[rinv[i]] <- 1/(max(raw[rinv[i]])+1-raw[rinv[i]])
}
# make function to fix scale() function so it returns a useful vector not a stupid matrix
scale2 <- function(x){as.vector(scale(x, scale = FALSE))}
# scale all numeric variables
d2 <- d %>% mutate(across(.cols=duration:peakf, .fns = scale2))
# filter out calls without fundfreq measures
d3 <- d2 %>%
filter(indicator == TRUE)
write.csv(d3, "/Users/jkvrtilek/Desktop/timepercent_alltimelog.csv")
rm(list = ls.str(mode = 'character'))
rm(list = ls.str(mode = 'numeric'))
rm(d)
rm(d2)
rm(raw)
raw <- d3
rm(d3)
# clear workspace
rm(list=ls())
raw <- read.csv(/Users/jkvrtilek/Desktop/timepercent_alltimelog.csv")
raw <- read.csv("/Users/jkvrtilek/Desktop/timepercent_alltimelog.csv")
# get sample sizes
d <- raw %>%
separate(sound.files, into=c('ds','date', 'bat', 'file', 'call'), sep="_", remove = FALSE) %>%
group_by(bat) %>%
mutate(sample.size= n()) %>%
ungroup()
# look at sample sizes
sort(unique(d$sample.size))
# set minimum sample size
# good rule of thumb is 20 obs per variable
predictors <- ncol(raw) - 3
min.sample.size <- predictors * 20
# filter calls by min sample size
d2 <- d %>%
filter(sample.size > min.sample.size)
# classify calls to bat using dfa with cross-validation (leave one-out classification)
dfa <- lda(bat ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf+
maxslope+
minslope+
abs_minslope+
pos_slopes+
neg_slopes+
turns+
meanslope+
segments,
CV=T,
data=d2)
# get classification matrix
cm <- table(d2$bat, dfa$class)
# get overall correct classification rate (accuracy)
# this is the best accuracy estimate
correct.cases <- sum(diag(cm))
all.cases <- sum(cm)
accuracy <- correct.cases/all.cases
accuracy
# get mean and range of correct assignments/all assignments to each bat
range(diag(cm)/colSums(cm), na.rm=T)
mean(diag(cm)/colSums(cm), na.rm=T)
median(diag(cm)/colSums(cm), na.rm=T)
# get mean and range of correct assignments/all calls from each bat
range(diag(cm)/rowSums(cm), na.rm=T)
mean(diag(cm)/rowSums(cm), na.rm=T)
median(diag(cm)/rowSums(cm), na.rm=T)
