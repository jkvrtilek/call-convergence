predictors <- ncol(raw) - 3
min.sample.size <- predictors * 20
# filter calls by min sample size
d2 <- d %>%
filter(sample.size > min.sample.size)
# classify calls to bat using dfa with cross-validation (leave one-out classification)
dfa <- lda(bat ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf+
maxslope+
minslope+
abs_minslope+
pos_slopes+
neg_slopes+
turns+
meanslope+
segments,
CV=T,
data=d2)
# get classification matrix
cm <- table(d2$bat, dfa$class)
# get overall correct classification rate (accuracy)
# this is the best accuracy estimate
correct.cases <- sum(diag(cm))
all.cases <- sum(cm)
accuracy <- correct.cases/all.cases
accuracy
rates19 <- readRDS("social_data/2019_grooming_rates.RDS")
View(rates19)
library(diffr)
diffr("/Users/jkvrtilek/Desktop/OSU/PhD/Calls/ff_dfa_test3_postorg.R","/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/call-convergence/03_individual_DFA.R")
# clear workspace
rm(list=ls())
# set working directory
setwd("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/call-convergence")
# load packages
library(MASS)
library(tidyverse)
# get data
raw <- readRDS("vampire_call_measures_filtered_transformed.RDS")
# get sample sizes
d <- raw %>%
separate(sound.files, into=c('ds','date', 'bat', 'file', 'call'), sep="_", remove = FALSE) %>%
group_by(bat) %>%
mutate(sample.size= n()) %>%
ungroup()
View(d)
# get sample sizes
d <- raw %>%
separate(sound.files, into=c('date', 'bat', 'file', 'call'), sep="_") %>%
group_by(bat) %>%
mutate(sample.size= n()) %>%
ungroup()
# linear discriminant function analysis to classify calls to bat
# Gerry Carter, gcarter1640@gmail.com
# updated by Julia Vrtilek 29 March 2024
# clear workspace
rm(list=ls())
# set working directory
setwd("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/call-convergence")
# load packages
library(MASS)
library(tidyverse)
# get data
raw <- readRDS("vampire_call_measures_filtered_transformed.RDS")
# get sample sizes
d <- raw %>%
separate(sound.files, into=c('ds','date', 'bat', 'file', 'call'), sep="_", remove = FALSE) %>%
group_by(bat) %>%
mutate(sample.size= n()) %>%
ungroup()
# look at sample sizes
sort(unique(d$sample.size))
# set minimum sample size
# good rule of thumb is 20 obs per variable
predictors <- ncol(raw) - 3
min.sample.size <- predictors * 20
# filter calls by min sample size
d2 <- d %>%
filter(sample.size > min.sample.size)
# classify calls to bat using dfa with cross-validation (leave one-out classification)
dfa <- lda(bat ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf+
maxslope+
minslope+
abs_minslope+
pos_slopes+
neg_slopes+
turns+
meanslope+
segments,
CV=T,
data=d2)
# get classification matrix
cm <- table(d2$bat, dfa$class)
# get overall correct classification rate (accuracy)
# this is the best accuracy estimate
correct.cases <- sum(diag(cm))
all.cases <- sum(cm)
accuracy <- correct.cases/all.cases
accuracy
# plot accuracy as a function of sampling effort
tibble(bat= colnames(cm),
accuracy= diag(cm)/colSums(cm),### I used column sums rather than row sums
n.cases= colSums(cm)) %>% ###
ggplot(aes(x=log(n.cases), y=accuracy))+
geom_point(size=2)+
geom_smooth(method= "lm")+
xlab("log10 number of cases")+
ylab('correct classification rate')+
ggtitle('accuracy by sampling effort per bat')
# save classification matrix
write.csv(cm, "dfa-cv-results-standard.csv")
# plot accuracy as a function of sampling effort
tibble(bat= colnames(cm),
accuracy= diag(cm)/colSums(cm),### I used column sums rather than row sums
n.cases= colSums(cm)) %>% ###
ggplot(aes(x=log(n.cases), y=accuracy))+
geom_point(size=2)+
geom_smooth(method= "lm")+
xlab("log10 number of cases")+
ylab('correct classification rate')+
ggtitle('accuracy by sampling effort per bat')
# classify calls to bat using a single dfa without cross validation
# this method can give elevated accuracy (but doesn't matter because we are using this to get cross-classification rates)
dfa2 <- lda(bat ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf,
CV=F,
data=d2)
# get classification rates
predictions <- predict(dfa2)
d2$prediction <- predictions$class
cm2 <- table(d2$bat, d2$prediction)
# get overall correct classification rate
correct.cases <- sum(diag(cm2))
all.cases <- sum(cm2)
accuracy2 <- correct.cases/all.cases
accuracy2
# get vocal distance as Mahalanobis distance between group centroids
meanGroup <- dfa2$means
distance <- as.matrix(dist(meanGroup %*% dfa2$scaling))
# save vocal similarity
write.csv(distance, file= "vocal-distance-lda.csv", row.names=F)
# load DFA distances
dist <- read_csv("vocal-distance-lda.csv")
View(dist)
unmatrix(dist)
View(dist)
enframe(unmatrix(dist))
row.names(dist) <- colnames(dist)
View(dist)
enframe(unmatrix(dist))
View(dist)
names(vlist)
enframe(unmatrix(dist))
traceback()
# get grooming data
load("2019_cleaning/2019_grooming_rates.Rdata")
setwd("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/call-convergence/social_data/")
library(tidyverse)
library(gdata)
# get grooming data
load("2019_cleaning/2019_grooming_rates.Rdata")
View(grooming_network)
str(grooming_network)
# load DFA distances
dist <- read_csv("vocal-distance-lda.csv")
# set working directory
setwd("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/call-convergence/")
# load DFA distances
dist <- read_csv("vocal-distance-lda.csv")
str(dist)
dist2 <- as.matrix(dist)
str(dist2)
rownames(dist2) <- colnames(dist2)
View(dist2)
enframe(unmatrix(dist2))
# load DFA distances
dist <- as.matrix(read_csv("vocal-distance-lda.csv"))
rownames(dist) <- colnames(dist)
dist2 <- enframe(unmatrix(dist))
View(dist2)
colnames(distdf) <- c(dir.dyad,dist)
# combine data from 2015, 2017, 2019, and call distance to make a dataframe
# columns: Directed dyad, Vocal distance, Kinship, Allogrooming given, Allogrooming received, Food given, Food received, Capture population bat 1, Capture population bat 2
# Julia Vrtilek
# 29 March 2024
# set working directory
setwd("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/call-convergence/")
# load packages
library(tidyverse)
library(gdata)
# load data
kin <- read_csv("social_data/kinship-all-bats.csv") %>%
mutate(dir.dyad = paste(bat1,bat2,sep="_")) %>%
select(dir.dyad,bat1,bat2,4:8)
rates15 <- read_csv("social_data/2015_foodsharing_rates.csv") %>%
mutate(dir.dyad = paste(donor,receiver,sep="_")) %>%
mutate(grooming = NA) %>%
mutate(foodsharing = rate) %>%
select(dir.dyad,grooming,foodsharing)
rates17 <- read_csv("social_data/2017_rates.csv") %>%
mutate(grooming = grooming.bond) %>%
mutate(foodsharing = sharing.bond) %>%
select(dir.dyad,grooming,foodsharing)
rates19 <- readRDS("social_data/2019_grooming_rates.RDS")
# combine rates for left_join
all_rates <- rbind(rates15,rates17)
# load DFA distances
dist <- as.matrix(read_csv("vocal-distance-lda.csv"))
rownames(dist) <- colnames(dist)
distdf <- enframe(unmatrix(dist))
colnames(distdf) <- c("dir.dyad","dist")
View(distdf)
View(distdf)
View(dist)
# make final df
d <- left_join(kin,all_rates,by = "dir.dyad")
d2 <- left_join(d,distdf,by = "dir.dyad")
View(d2)
# load DFA distances
dist <- as.matrix(read_csv("vocal-distance-lda.csv"))
rownames(dist) <- colnames(dist)
distdf <- enframe(unmatrix(dist))
distdf$dyad <- gsub(":","_",d$dyad)
# load DFA distances
dist <- as.matrix(read_csv("vocal-distance-lda.csv"))
rownames(dist) <- colnames(dist)
distdf <- enframe(unmatrix(dist))
colnames(distdf) <- c("dir.dyad","dist")
distdf$dir.dyad <- gsub(":","_",distdf$dir.dyad)
View(distdf)
# make final df
d <- left_join(kin,all_rates,by = "dir.dyad")
d2 <- left_join(d,distdf,by = "dir.dyad")
View(d2)
View(d2)
hist(distdf$dist)
View(distdf)
nonzero <- distdf %>% filter(dist != 0)
View(nonzero)
hist(nonzero$dist)
library(nortest)
ad.test(nonzero$dist)
# linear discriminant function analysis to classify calls to bat
# Gerry Carter, gcarter1640@gmail.com
# updated by Julia Vrtilek 29 March 2024
# clear workspace
rm(list=ls())
# set working directory
setwd("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/call-convergence")
# load packages
library(MASS)
library(tidyverse)
# get data
raw <- readRDS("vampire_call_measures_filtered_transformed.RDS")
# get sample sizes
d <- raw %>%
separate(sound.files, into=c('ds','date', 'bat', 'file', 'call'), sep="_", remove = FALSE) %>%
group_by(bat) %>%
mutate(sample.size= n()) %>%
ungroup()
# look at sample sizes
sort(unique(d$sample.size))
# set minimum sample size
# good rule of thumb is 20 obs per variable
predictors <- ncol(raw) - 3
min.sample.size <- predictors * 20
# filter calls by min sample size
d2 <- d %>%
filter(sample.size > min.sample.size)
# classify calls to bat using dfa with cross-validation (leave one-out classification)
dfa <- lda(bat ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf+
maxslope+
minslope+
abs_minslope+
pos_slopes+
neg_slopes+
turns+
meanslope+
segments,
CV=T,
data=d2)
# get classification matrix
cm <- table(d2$bat, dfa$class)
# get overall correct classification rate (accuracy)
# this is the best accuracy estimate
correct.cases <- sum(diag(cm))
all.cases <- sum(cm)
accuracy <- correct.cases/all.cases
accuracy
# get mean and range of correct assignments/all assignments to each bat
range(diag(cm)/colSums(cm), na.rm=T)
mean(diag(cm)/colSums(cm), na.rm=T)
# get mean and range of correct assignments/all calls from each bat
range(diag(cm)/rowSums(cm), na.rm=T)
mean(diag(cm)/rowSums(cm), na.rm=T)
# save classification matrix
write.csv(cm, "dfa-cv-results-standard.csv")
# plot accuracy as a function of sampling effort
tibble(bat= colnames(cm),
accuracy= diag(cm)/colSums(cm),### I used column sums rather than row sums
n.cases= colSums(cm)) %>% ###
ggplot(aes(x=log(n.cases), y=accuracy))+
geom_point(size=2)+
geom_smooth(method= "lm")+
xlab("log10 number of cases")+
ylab('correct classification rate')+
ggtitle('accuracy by sampling effort per bat')
# classify calls to bat using a single dfa without cross validation
# this method can give elevated accuracy (but doesn't matter because we are using this to get cross-classification rates)
dfa2 <- lda(bat ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf,
CV=F,
data=d2)
# get classification rates
predictions <- predict(dfa2)
d2$prediction <- predictions$class
cm2 <- table(d2$bat, d2$prediction)
# get overall correct classification rate
correct.cases <- sum(diag(cm2))
all.cases <- sum(cm2)
accuracy2 <- correct.cases/all.cases
accuracy2
# get vocal distance as Mahalanobis distance between group centroids
meanGroup <- dfa2$means
distance <- as.matrix(dist(meanGroup %*% dfa2$scaling))
# save vocal similarity
write.csv(distance, file= "vocal-distance-lda.csv", row.names=F)
# get proportion of variance explained by the discriminant functions
props.df <- dfa2$svd*100/sum(dfa2$svd)
tibble(prop= props.df, x= 1:length(props.df)) %>%
ggplot(aes(x=x, y=prop))+
geom_col()+
xlab("discriminant function")+
ylab("proportion of variance explained")
# and the coefficients of the linear discriminant functions (these tell you which variables were most important for identifying cases)
dfa2$scaling
# save all DFA loadings sorted by absolute value of DF1
loadings <-
dfa2$scaling %>%
as.data.frame() %>%
arrange(desc(abs(LD1)))
write.csv(loadings, "dfa-loadings.csv")
props.df
sum(props.df[1:5])
sum(props.df[1:10])
sum(props.df[1:20])
sum(props.df[1:27])
# look at correlation between accuracy rates per bat (cross-validated or not)
n <- rowSums(cm)
c1 <- diag(cm)/n
c2 <- diag(cm2)/n
tibble(c1,c2, n) %>%
ggplot(aes(x=c2, y=c1))+
geom_point(size=2)+
geom_smooth(method="lm")+
xlab("training accuracy")+
ylab("testing accuracy")
rates19 <- readRDS("social_data/2019_grooming_rates.RDS")
View(rates19)
load("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/call-convergence/social_data/2019_cleaning/2019_grooming_rates.Rdata")
grooming_network
View(grooming_network)
# load data
kin <- read_csv("social_data/kinship-all-bats.csv") %>%
mutate(dir.dyad = paste(bat1,bat2,sep="_")) %>%
select(dir.dyad,bat1,bat2,4:8)
View(kin)
load("/Users/jkvrtilek/Downloads/rates_adoption.Rdata")
View(rates_adoption)
load("/Users/jkvrtilek/Downloads/rates2019_2021-05-18.Rdata")
View(rates2019)
# load DFA distances
dist <- as.matrix(read_csv("vocal-distance-lda.csv"))
rownames(dist) <- colnames(dist)
distdf <- enframe(unmatrix(dist))
colnames(distdf) <- c("dir.dyad","dist")
distdf$dir.dyad <- gsub(":","_",distdf$dir.dyad)
View(distdf)
hist(log(distdf$dist))
ad.test(log(distdf$dist))
