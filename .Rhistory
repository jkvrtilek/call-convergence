median(diag(cm)/colSums(cm), na.rm=T)
# get mean and range of correct assignments/all calls from each bat
range(diag(cm)/rowSums(cm), na.rm=T)
mean(diag(cm)/rowSums(cm), na.rm=T)
median(diag(cm)/rowSums(cm), na.rm=T)
# classify calls to bat using a single dfa without cross validation
# this method can give elevated accuracy (but doesn't matter because we are using this to get cross-classification rates)
dfa2 <- lda(bat ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf,
CV=F,
data=d2)
# save all DFA loadings sorted by absolute value of DF1
loadings <-
dfa2$scaling %>%
as.data.frame() %>%
arrange(desc(abs(LD1)))
write.csv(loadings, "/Users/jkvrtilek/Desktop/allnonzeromin-loadings.csv")
raw <- read.csv("/Users/jkvrtilek/Desktop/no_time_zeroes_transform.csv")
# get sample sizes
d <- raw %>%
separate(sound.files, into=c('ds','date', 'bat', 'file', 'call'), sep="_", remove = FALSE) %>%
group_by(bat) %>%
mutate(sample.size= n()) %>%
ungroup()
# look at sample sizes
sort(unique(d$sample.size))
# set minimum sample size
# good rule of thumb is 20 obs per variable
predictors <- ncol(raw) - 3
min.sample.size <- predictors * 20
# filter calls by min sample size
d2 <- d %>%
filter(sample.size > min.sample.size)
# classify calls to bat using dfa with cross-validation (leave one-out classification)
dfa <- lda(bat ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf+
maxslope+
minslope+
abs_minslope+
pos_slopes+
neg_slopes+
turns+
meanslope+
segments,
CV=T,
data=d2)
# classify calls to bat using dfa with cross-validation (leave one-out classification)
dfa <- lda(bat ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf+
maxslope+
minslope+
abs_minslope+
pos_slopes+
neg_slopes+
turns+
meanslope+
segments,
CV=T,
data=d2)
# get classification matrix
cm <- table(d2$bat, dfa$class)
# get overall correct classification rate (accuracy)
# this is the best accuracy estimate
correct.cases <- sum(diag(cm))
all.cases <- sum(cm)
accuracy <- correct.cases/all.cases
accuracy
# get mean and range of correct assignments/all assignments to each bat
range(diag(cm)/colSums(cm), na.rm=T)
mean(diag(cm)/colSums(cm), na.rm=T)
median(diag(cm)/colSums(cm), na.rm=T)
# get mean and range of correct assignments/all calls from each bat
range(diag(cm)/rowSums(cm), na.rm=T)
mean(diag(cm)/rowSums(cm), na.rm=T)
median(diag(cm)/rowSums(cm), na.rm=T)
View(raw)
sf <- raw$sound.files
View(raw)
View(d)
unique(d$bat)
unique(d2$bat)
test <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/LFS/vampire_call_measures_filtered.csv")
test2 <- test %>% separate(sound.files, into=c('ds','date', 'bat', 'file', 'call'), sep="_", remove = FALSE) %>%
group_by(bat) %>%
mutate(sample.size= n()) %>%
ungroup()
test3 <- filter(sample.size > min.sample.size)
test3 <- test2 %>% filter(sample.size > min.sample.size)
unique(test3$bat)
batsA <- unique(test3$bat)
batsB <- unique(d2$bat)
batsA[batsA %in$ batsB]
batsA[batsA %in% batsB]
batsA[batsA !%in% batsB]
batsA[batsA %nin% batsB]
batsA[!(batsA %nin% batsB)]
batsA[!(batsA %in% batsB)]
xxx <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/LFS/vampire_call_measures_filtered_transformed.csv")
xxx2 <- xxx %>%
separate(sound.files, into=c('ds','date', 'bat', 'file', 'call'), sep="_", remove = FALSE) %>%
group_by(bat) %>%
mutate(sample.size= n()) %>%
ungroup()
xxx3 <- xxx2 %>% filter(sample.size > min.sample.size)
batsA_trans <- unique(xxx3$bat)
batsA_trans[!(batsA_trans %in% batsB)]
rm(xxx)
rm(xxx2)
rm(xxx3)
rm(test)
rm(test2)
rm(test3)
raw <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/LFS/vampire_call_measures_filtered.csv")
View(raw)
library(warbleR)
install.packages("warbleR")
library(tuneR)
test <- readWave("/Users/jkvrtilek/Desktop/OSU/PhD/Calls/vampire_bat_calls/lucy/2011-08-17/2011-08-17_lucy_T0000714.WAV")
spectro(test)
library(seewave)
spectro(test)
View(raw)
test <- readWave("/Users/jkvrtilek/Desktop/OSU/PhD/Calls/vampire_bat_calls/mya/2011-08-15/2011-08-15_mya_T0000438.WAV")
spectro(test)
test <- readWave("/Users/jkvrtilek/Desktop/OSU/PhD/Calls/vampire_bat_calls/cerceson/2013-02-17/2013-02-18_cerceson_T0006585.WAV")
spectro(test)
test <- readWave("/Users/jkvrtilek/Desktop/OSU/PhD/Calls/vampire_bat_calls/dcd/2016-05-15/2016-05-15_dcd_T0037717.WAV")
spectro(test)
test <- readWave("/Users/jkvrtilek/Desktop/OSU/PhD/Calls/vampire_bat_calls/piper/2019-10-02/2019-10-03_piper_T0067396.WAV")
spectro(test)
test <- readWave("/Users/jkvrtilek/Desktop/OSU/PhD/Calls/vampire_bat_calls/vixen/2019-10-08/2019-10-08_vixen_T0073340.WAV")
spectro(test)
# data cleaning: relabel files, s to ms, duration and peak freq filters, add binary ff variable
# Julia Vrtilek
# 23 March 2023
# set working directory
setwd("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/LFS")
# load packages
library(tidyverse)
# load data
raw <- read.csv("vampire_call_measures.csv")
# rename files mistakenly labeled from 2018
d <- raw
length(grep("2018-", raw$sound.files))
d$sound.files <- gsub("2018-08-29","2017-09-03", raw$sound.files)
length(grep("2018-", d$sound.files))
# convert seconds to milliseconds
d2 <- d
d2$duration <- d$duration * 1000
d2$time.median <- d$time.median * 1000
d2$time.Q25 <- d$time.Q25 * 1000
d2$time.Q75 <- d$time.Q75 * 1000
d2$time.IQR <- d$time.IQR * 1000
# filters: duration, peak frequency, time variables
d3 <- d2 %>%
filter(duration > 3) %>%
filter(duration < 50) %>%
filter(peakf > 10) %>%
filter(peakf < 30) %>%
filter(time.Q25 > 0) %>%
filter(time.median > 0) %>%
filter(time.Q75 > 0) %>%
filter(time.IQR > 0)
# add binary variable for whether fundamental frequency measures succeeded
d4 <- d3 %>%
mutate(indicator = case_when(!is.na(meanslope) ~ T,
is.na(meanslope) ~ F,
is.nan(meanslope) ~ F))
# save file
write.csv(d4, "vampire_call_measures_filtered.csv")
# script to transform data, then scale it for DFA
# Julia Vrtilek
# 24 March 2024
# set working directory
setwd("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/LFS")
# load packages
library(tidyverse)
library(MASS)
# get data
raw <- read.csv("vampire_call_measures_filtered.csv")
# make df to modify
d <- raw
# temp make time variables into percentages
d$time.Q25 <- raw$time.Q25/raw$duration
d$time.median <- raw$time.median/raw$duration
d$time.Q75 <- raw$time.Q75/raw$duration
d$time.IQR <- raw$time.IQR/raw$duration
# # how to transform variables
# wsqrt <- c("dfrange","abs_minslope","pos_slopes","neg_slopes")
# wlog <- c("duration","time.median","time.Q25","time.Q75","time.IQR","skew","kurt","sfm","mindom","enddom","meanpeakf","turns")
# winv <- c("freq.Q25","meandom","modindx")
# rsqrt <- c("freq.median")
# rlog <- c("freq.Q75","freq.IQR","segments")
# rinv <- c("sp.ent","time.ent","entropy")
# none <- c("meanfreq","sd","maxdom","startdom","dfslope","peakf","maxslope","minslope","meanslope")
#
# # do transformations
# # note: k = max + 1
#
# # square root
# for (i in 1:length(wsqrt)) {
#   d[wsqrt[i]] <- sqrt(raw[wsqrt[i]])
# }
#
# # log
# for (i in 1:length(wlog)) {
#   d[wlog[i]] <- log(raw[wlog[i]] +
#                     min(raw[wlog[i]][raw[wlog[i]] > 0], na.rm = T))
# }
#
# # inverse
# for (i in 1:length(winv)) {
#   d[winv[i]] <- 1/raw[winv[i]]
# }
#
# # reflect and root
# for (i in 1:length(rsqrt)) {
#   d[rsqrt[i]] <- sqrt(max(raw[rsqrt[i]]) +
#                       min(raw[rsqrt[i]][raw[rsqrt[i]] > 0], na.rm = T) -
#                       raw[rsqrt[i]])
# }
#
# # reflect and log
# for (i in 1:length(rlog)) {
#   d[rlog[i]] <- log(max(raw[rlog[i]],na.rm=T) +
#                     min(raw[rlog[i]][raw[rlog[i]] > 0], na.rm = T) -
#                     raw[rlog[i]])
# }
#
# # reflect and inverse
# for (i in 1:length(rinv)) {
#   d[rinv[i]] <- 1/(max(raw[rinv[i]]) +
#                    min(raw[rinv[i]][raw[rinv[i]] > 0], na.rm = T) -
#                    raw[rinv[i]])
# }
# make function to fix scale() function so it returns a useful vector not a stupid matrix
scale2 <- function(x){as.vector(scale(x, scale = FALSE))}
# scale all numeric variables
d2 <- d %>% mutate(across(.cols=duration:peakf, .fns = scale2))
# filter out calls without fundfreq measures
d3 <- d2 %>%
filter(indicator == TRUE)
write.csv(d3, "vampire_call_measures_filtered_transformed.csv")
# # # where indicator is FALSE, turn all fund freq measurements to 0
# # ff <- colnames(d2)[30:37]
# #
# # for (i in 1:nrow(d2)) {
# #   if (d2[i,"indicator"] == FALSE) {
# #     for (j in 1:length(ff)) {
# #       d2[i,ff[j]] <- 0
# #     }
# #     print(i)
# #   }
# # }
# linear discriminant function analysis to classify calls to bat
# Gerry Carter, gcarter1640@gmail.com
# updated by Julia Vrtilek 29 March 2024
# clear workspace
rm(list=ls())
# set working directory
setwd("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/call-convergence")
# load packages
library(MASS)
library(tidyverse)
# get data
raw <- read.csv("vampire_call_measures_filtered_transformed.csv")
raw <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/LFS/vampire_call_measures_filtered_transformed.csv")
# clear workspace
rm(list=ls())
# get sample sizes
d <- raw %>%
separate(sound.files, into=c('ds','date', 'bat', 'file', 'call'), sep="_", remove = FALSE) %>%
group_by(bat) %>%
mutate(sample.size= n()) %>%
ungroup()
raw <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/LFS/vampire_call_measures_filtered_transformed.csv")
# get sample sizes
d <- raw %>%
separate(sound.files, into=c('ds','date', 'bat', 'file', 'call'), sep="_", remove = FALSE) %>%
group_by(bat) %>%
mutate(sample.size= n()) %>%
ungroup()
# look at sample sizes
sort(unique(d$sample.size))
# set minimum sample size
# good rule of thumb is 20 obs per variable
predictors <- ncol(raw) - 3
min.sample.size <- predictors * 20
# filter calls by min sample size
d2 <- d %>%
filter(sample.size > min.sample.size)
unique(d2$bat)
# classify calls to bat using dfa with cross-validation (leave one-out classification)
dfa <- lda(bat ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf+
maxslope+
minslope+
abs_minslope+
pos_slopes+
neg_slopes+
turns+
meanslope+
segments,
CV=T,
data=d2)
# get classification matrix
cm <- table(d2$bat, dfa$class)
# get overall correct classification rate (accuracy)
# this is the best accuracy estimate
correct.cases <- sum(diag(cm))
all.cases <- sum(cm)
accuracy <- correct.cases/all.cases
accuracy
# get mean and range of correct assignments/all assignments to each bat
range(diag(cm)/colSums(cm), na.rm=T)
mean(diag(cm)/colSums(cm), na.rm=T)
median(diag(cm)/colSums(cm), na.rm=T)
# get mean and range of correct assignments/all calls from each bat
range(diag(cm)/rowSums(cm), na.rm=T)
mean(diag(cm)/rowSums(cm), na.rm=T)
median(diag(cm)/rowSums(cm), na.rm=T)
# save classification matrix
write.csv(cm, "dfa-cv-results-standard.csv")
# plot accuracy as a function of sampling effort
tibble(bat= colnames(cm),
accuracy= diag(cm)/colSums(cm),### I used column sums rather than row sums
n.cases= colSums(cm)) %>% ###
ggplot(aes(x=log(n.cases), y=accuracy))+
geom_point(size=2)+
geom_smooth(method= "lm")+
xlab("log10 number of cases")+
ylab('correct classification rate')+
ggtitle('accuracy by sampling effort per bat')
# classify calls to bat using a single dfa without cross validation
# this method can give elevated accuracy (but doesn't matter because we are using this to get cross-classification rates)
dfa2 <- lda(bat ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf,
CV=F,
data=d2)
# get classification rates
predictions <- predict(dfa2)
d2$prediction <- predictions$class
cm2 <- table(d2$bat, d2$prediction)
# get overall correct classification rate
correct.cases <- sum(diag(cm2))
all.cases <- sum(cm2)
accuracy2 <- correct.cases/all.cases
accuracy2
# get vocal distance as Mahalanobis distance between group centroids
meanGroup <- dfa2$means
distance <- as.matrix(dist(meanGroup %*% dfa2$scaling))
# save vocal similarity
write.csv(distance, file= "vocal-distance-lda.csv", row.names=F)
# get proportion of variance explained by the discriminant functions
props.df <- dfa2$svd*100/sum(dfa2$svd)
tibble(prop= props.df, x= 1:length(props.df)) %>%
ggplot(aes(x=x, y=prop))+
geom_col()+
xlab("discriminant function")+
ylab("proportion of variance explained")
# and the coefficients of the linear discriminant functions (these tell you which variables were most important for identifying cases)
dfa2$scaling
# save all DFA loadings sorted by absolute value of DF1
loadings <-
dfa2$scaling %>%
as.data.frame() %>%
arrange(desc(abs(LD1)))
write.csv(loadings, "dfa-loadings.csv")
# look at correlation between accuracy rates per bat (cross-validated or not)
n <- rowSums(cm)
c1 <- diag(cm)/n
c2 <- diag(cm2)/n
tibble(c1,c2, n) %>%
ggplot(aes(x=c2, y=c1))+
geom_point(size=2)+
geom_smooth(method="lm")+
xlab("training accuracy")+
ylab("testing accuracy")
cor.test(c1,c2)
# set working directory
setwd("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/call-convergence/")
detach("package:MASS", unload = TRUE)
